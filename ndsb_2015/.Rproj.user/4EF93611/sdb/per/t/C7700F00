{
    "contents" : "## Starter Code for Analyzing Plankton Data\n\n## Created by Jeff Hebert 12/30/14\n## Kaggle Competition: http://www.kaggle.com/c/datasciencebowl\n\n## This is basic starter code to create a model to classify plankton\n## based on image dimensions and image density.\n## This code should produce a results file with public score of 3.397518.\n## Surely you can come up with a better model!\n\n\n## Instructions\n## 1. Download and unzip the competition data\n## 2. Set the data_dir variable to the path to the data directory\n## 3. Run the code. It will take 30-50 minutes, limited by IO of the thousands of files\n## 4. Customize the extract_stats function to add your custom variables\n## 4a. Remember to add new variables to train_data and test_data\n\n\n## Hints\n## Use cross validation to get a better model.\n## Please let me know if you find this starter code helpful.\n## Remember to use Rprof() and microbenchmark() when your code slows down.\n## Remember to use some form of version control and rename your submission files.\n\n\n## ==============================\n## Main variables\n## ==============================\n\n\n## Set the path to your data here\ndata_dir <- \"c://data/plankton\"\n\ntrain_data_dir <- paste(data_dir,\"/train\", sep=\"\")\ntest_data_dir <- paste(data_dir,\"/test\", sep=\"\")\n\n\n\n## ==============================\n## Load packages\n## ==============================\n\n## These packages are available from CRAN\n\nlibrary(jpeg)\nlibrary(randomForest)\n\n\n## ==============================\n## Define Functions\n## ==============================\n\n## Handy function to display a greyscale image of the plankton\nim <- function(image) image(image, col = grey.colors(32))\n\n\n## Function to extract some simple statistics about the image\n## UPDATE THIS to do any calculations that you think are useful\nextract_stats <- function(working_image = working_image){\n    #Invert the image to calculate density\n  \n#   image <- working_image < mean(working_image) # original code\n    \n    ### MY TESTING CODE ##\n    image <- working_image > mean(working_image)\n    image <- ifelse(image == TRUE, 0, 1)\n    kern = makeBrush(3, shape='diamond')\n    image <- dilate(image, kern)\n    image <- bwlabel(image)\n    ## MY CODE ABOVE ##\n    \n    im_length <- nrow(image)\n    im_width <- ncol(image)\n    im_density <- mean(image)\n    im_ratio <- im_length / im_width \n    return(c(length=im_length,width=im_width,density=im_density,ratio=im_ratio))    \n}\n\n## Function to calculate multi-class loss on train data\nmcloss <- function (y_actual, y_pred) {\n    dat <- rep(0, length(y_actual))\n    for(i in 1:length(y_actual)){\n        dat_x <- y_pred[i,y_actual[i]]\n        dat[i] <- min(1-1e-15,max(1e-15,dat_x))\n    }\n    return(-sum(log(dat))/length(y_actual))\n}\n\n## ==============================\n## Read training data\n## ==============================\n\n\n## Create empty data structure to store summary statistics from each image file\ntrain_data <- data.frame(class = character(), filename = character(),lenght=numeric(),width=numeric(),density=numeric(),ratio=numeric(), stringsAsFactors = FALSE)\n\n\n## Get list of classes\nclasses <- list.dirs(train_data_dir, full.names = FALSE)\nclasses <- setdiff(classes,\"\")\n\n## Read all the image files and calculate training statistics\n\nfor(classID in classes){\n    # Get list of all the examples of this classID\n    train_file_list <- list.files(paste(train_data_dir,\"/\",classID,sep=\"\"))\n    train_cnt <- length(train_file_list)\n    working_data <- data.frame(class = rep(\"a\",train_cnt), filename = \"a\",lenght=0,width=0,density=0,ratio=0, stringsAsFactors = FALSE)\n    idx <- 1\n    #Read and process each image\n    for(fileID in train_file_list){\n        working_file <- paste(train_data_dir,\"/\",classID,\"/\",fileID,sep=\"\")\n        working_image <- readJPEG(working_file)\n        \n        # Calculate model statistics \n        \n        working_stats <- extract_stats(working_image)\n        working_summary <- array(c(classID,fileID,working_stats))\n        working_data[idx,] <- working_summary\n        idx <- idx + 1\n    }\n    train_data <- rbind(train_data,working_data)\n    cat(\"Finished processing\",classID, '\\n')\n}\n\n## make life easier\nrequire(dplyr)\ntrain <- sample_frac(train_data, 0.05)\n\n## ==============================\n## Create Model\n## ==============================\n\n## We need to convert class to a factor for randomForest\n## so we might as well get subsets of x and y data for easy model building\ny_dat <- as.factor(train$class)\nx_dat <- train[,3:6]\n\nplankton_model <- randomForest(y = y_dat, x = x_dat, ntree=100, mtry=3)\n\nrequire(caret)\ndata <- data.frame(y = y_dat, x = x_dat)\n\n\nxx <- train(y = y_dat, x = x_dat, method=\"rf\")\n\n# Compare importance of the variables\nimportance(plankton_model)\n\n\n## Check overall accuracy... 24%, not very good but not bad for a simplistic model\ntable(plankton_model$predicted==y_dat)\n#  FALSE  TRUE \n#  22959  7377 # original\n# FALSE  TRUE \n# 23961  6371 # modified 1\n\n## Make predictions and calculate log loss\ny_predictions <- predict(plankton_model, type=\"prob\")\n\nymin <- 1/1000\ny_predictions[y_predictions<ymin] <- ymin\n\nmcloss(y_actual = y_dat, y_pred = y_predictions)\n# 3.362268 # original\n# 3.596299 # modified 1\n\n## ==============================\n## Read test data and make predictions\n## ==============================\n\n\n\n## Read all the image files and calculate training statistics\n## This should take about 10 minutes, with speed limited by IO of the thousands of files\n\n    # Get list of all the examples of this classID\n    test_file_list <- list.files(paste(test_data_dir,sep=\"\"))\n    test_cnt <- length(test_file_list)\n    test_data <- data.frame(image = rep(\"a\",test_cnt), lenght=0,width=0,density=0,ratio=0, stringsAsFactors = FALSE)\n    idx <- 1\n    #Read and process each image\n    for(fileID in test_file_list){\n        working_file <- paste(test_data_dir,\"/\",fileID,sep=\"\")\n        working_image <- readJPEG(working_file)\n        \n        # Calculate model statistics\n        \n        ## YOUR CODE HERE ##\n        working_stats <- extract_stats(working_image)\n        working_summary <- array(c(fileID,working_stats))\n        test_data[idx,] <- working_summary\n        idx <- idx + 1\n        if(idx %% 10000 == 0) cat('Finished processing', idx, 'of', test_cnt, 'test images', '\\n')\n    }\n\n\n## Make predictions with class probabilities\ntest_pred <- predict(plankton_model, test_data, type=\"prob\")\ntest_pred[test_pred<ymin] <- ymin\n\n## ==============================\n## Save Submission File\n## ==============================\n\n## Combine image filename and class predictions, then save as csv\nsubmission <- cbind(image = test_data$image, test_pred)\nsubmission_filename <- paste(data_dir,\"/submission03.csv\",sep=\"\")\nwrite.csv(submission, submission_filename, row.names = FALSE)\n\n\n",
    "created" : 1421184803585.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "555304783",
    "id" : "C7700F00",
    "lastKnownWriteTime" : 1421357861,
    "path" : "C:/Users/matthew_harris/Dropbox/GitHub/plankton_NDSB/plankton_starter_code_hebert.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}